services:
  llamacpp:
    image: {{ image }}:{{ version }}
    container_name: {{ service }}
    hostname: {{ service }}
    restart: unless-stopped
    
    # GPU support using runtime (compatible with older docker-compose)
    runtime: nvidia
    
    environment:
      - TZ=America/Los_Angeles
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_LAUNCH_BLOCKING=1
      - NVIDIA_REQUIRE_CUDA=cuda>=11.0
    
    ports:
      - "{{ port }}:{{ port }}"
    
    volumes:
      - {{ home }}/models:/models
      - {{ home }}/config:/config
      - {{ home }}/logs:/logs
    
    # RTX 3090 Optimized: 8B Llama-3.1 Instruct model with full GPU acceleration  
    command: 
      - "--host"
      - "0.0.0.0"
      - "--port" 
      - "{{ port }}"
      - "--model"
      - "/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
      - "--n-gpu-layers"
      - "-1"
      - "--ctx-size"
      - "16384"
      - "--parallel" 
      - "4"
      - "--batch-size"
      - "2048"
      - "--ubatch-size" 
      - "512"
      - "--flash-attn"
      - "--api-key"
      - "{{ api_key | default('llamacpp-homelab-key') }}"
      - "--verbose"
    
    # Health check - check if server is responding
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:{{ port }}/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    
    # User mapping
    user: "{{ uid }}:{{ gid }}"
    
    # Security options
    security_opt:
      - no-new-privileges:true
    
    # Use default bridge network mode (same as existing containers)
    network_mode: bridge
