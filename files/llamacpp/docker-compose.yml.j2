services:
  llamacpp:
    image: {{ image }}:{{ version }}
    container_name: {{ service }}
    hostname: {{ service }}
    restart: unless-stopped
    
    # GPU support with explicit device reservation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    
    environment:
      - TZ=America/Los_Angeles
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CUDA_VISIBLE_DEVICES=0
    
    ports:
      - "{{ port }}:{{ port }}"
    
    volumes:
      - {{ home }}/models:/models
      - {{ home }}/config:/config
      - {{ home }}/logs:/logs
    
    # RTX 3090 Optimized: Qwen3-14B for single-user reasoning with max context
    # Model: 8.5GB + ~7GB KV cache = fits in 16GB VRAM budget
    # Supports /think mode for step-by-step reasoning
    command: 
      - "--host"
      - "0.0.0.0"
      - "--port" 
      - "{{ port }}"
      - "--model"
      - "/models/Qwen3-14B-Q4_K_M.gguf"
      - "--n-gpu-layers"
      - "41"
      - "--ctx-size"
      - "40960"
      - "--parallel" 
      - "1"
      - "--batch-size"
      - "2048"
      - "--ubatch-size" 
      - "512"
      - "--flash-attn"
      - "--api-key"
      - "{{ api_key | default('llamacpp-homelab-key') }}"
      - "--metrics"
      - "--verbose"
    
    # Health check - check if server is responding
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:{{ port }}/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    
    # Note: Running as root for CUDA/GPU access
    # The llama.cpp container requires root for proper GPU memory allocation
    
    # Security options
    security_opt:
      - no-new-privileges:true
    
    # Use default bridge network mode (same as existing containers)
    network_mode: bridge
