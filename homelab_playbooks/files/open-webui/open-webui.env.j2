# Open WebUI Configuration Environment Variables
# This file contains environment variables for Open WebUI configuration

# Required core variables
WEBUI_SECRET_KEY={{ ai_services.open_webui.secret_key }}
SCARF_NO_ANALYTICS=true
DO_NOT_TRACK=true

# Timezone
TZ=America/Los_Angeles

# API Configuration - Connect to llama.cpp container via host IP
OPENAI_API_BASE_URL=http://{{ ansible_default_ipv4.address }}:8080/v1
OPENAI_API_KEY=dummy
ENABLE_OPENAI_API=true
ENABLE_OLLAMA_API=false

# Pre-configure API connections
OPENAI_API_KEYS=[{"api_key":"dummy","api_base_url":"http://{{ ansible_default_ipv4.address }}:8080/v1","api_name":"llama.cpp Local"}]
DEFAULT_OPENAI_API_BASE_URL=http://{{ ansible_default_ipv4.address }}:8080/v1
DEFAULT_OPENAI_API_KEY=dummy

# WebUI Configuration
WEBUI_NAME=Ocean AI Chat
WEBUI_URL=http://{{ ansible_default_ipv4.address }}:{{ port }}
DEFAULT_MODELS=Phi-3.5-mini-instruct-Q4_K_M.gguf

# Model caching and connections - enable by default
ENABLE_MODEL_FILTER=true
MODEL_FILTER_ENABLED=true
CACHE_BASE_MODEL_LIST=true
ENABLE_DIRECT_CONNECTIONS=true

# Database Configuration - SQLite (stable)
DATABASE_URL=sqlite:////data/webui.db
DATA_DIR=/data

# Authentication Settings
ENABLE_SIGNUP=true
ENABLE_LOGIN_FORM=true
ENABLE_COMMUNITY_SHARING=false
DEFAULT_USER_ROLE=user

# Security Settings
ENABLE_RAG_WEB_SEARCH=true
ENABLE_WEB_BROWSE=true
ENABLE_IMAGE_GENERATION=true
ENABLE_RAG_LOCAL_WEB_FETCH=false

# Chat Configuration
ENABLE_MODEL_FILTER=true
MODEL_FILTER_ENABLED=true
GLOBAL_RATE_LIMIT_ENABLED=false
USER_PERMISSIONS_CHAT_DELETION=true
USER_PERMISSIONS_CHAT_EDITING=true

# File Upload Settings
ENABLE_RAG=false
CHUNK_SIZE=1000
CHUNK_OVERLAP=100

# Admin Settings
SHOW_ADMIN_DETAILS=true
ADMIN_EMAIL={{ ansible_user_id }}@{{ ansible_domain | default('localhost') }}

# Logging
LOG_LEVEL=INFO
# Note: WEBUI_SECRET_KEY configured at top of file from vault

# Performance
TIMEOUT=120
REQUEST_SIZE_LIMIT=100
