---
- name: Configure llama.cpp API server with GPU support using Docker Compose
  hosts: ocean
  become: true
  gather_facts: true

  vars_files:
    - ../../../../vault/secrets.yaml
    - ../../../../vars/vars_llamacpp_models.yaml
    - ../../../../vars/vars_service_ports.yaml

  vars:
    service: llamacpp
    image: ghcr.io/ggerganov/llama.cpp
    version: server-cuda
    port: "{{ service_ports.llamacpp.port }}"
    data: /data01
    files: ../../../../files/llamacpp
    home: "{{ data }}/services/{{ service }}"
    user: media
    uid: 1001
    gid: 1001
    api_key: llamacpp-homelab-key
    # RTX 3090 optimized profile  
    selected_profile: max_performance

  tasks:

  - name: Ensure base directory exists
    ansible.builtin.file:
      path: "{{ home }}"
      state: directory
      owner: "{{ user }}"
      group: "{{ user }}"
      mode: '0755'

  - name: Ensure subdirectories exist
    ansible.builtin.file:
      path: "{{ home }}/{{ item }}"
      state: directory
      owner: "{{ user }}"
      group: "{{ user }}"
      mode: '0755'
    with_items:
    - models
    - config
    - logs

  - name: Debug HuggingFace token availability
    ansible.builtin.debug:
      msg: "HF Token available: {{ 'Yes' if (ai_services.huggingface.token is defined and ai_services.huggingface.token != '') else 'No' }}"
    tags:
      - models
      - debug

  - name: Download authenticated HuggingFace models (QwQ-32B) 
    ansible.builtin.get_url:
      url: "https://huggingface.co/{{ item.1.repo_id }}/resolve/main/{{ item.1.filename }}"
      dest: "{{ home }}/models/{{ item.1.name }}"
      mode: '0644'
      owner: "{{ user }}"
      group: "{{ user }}"
      timeout: "{{ item.1.timeout | default(3600) }}"
      force: false
      headers:
        Authorization: "Bearer {{ ai_services.huggingface.token }}"
    with_subelements:
      - "{{ llamacpp_models | dict2items }}"
      - value
    when: 
      - item.1.requires_auth | default(false)
      - item.1.priority <= 2
      - ai_services.huggingface.token is defined
      - ai_services.huggingface.token != ''
    tags: 
      - models
      - download
      - auth
    retries: 3
    delay: 10
    register: hf_download_result

  - name: Download public models (fallback and others)
    ansible.builtin.get_url:
      url: "{{ item.1.url }}"
      dest: "{{ home }}/models/{{ item.1.name }}"
      mode: '0644'
      owner: "{{ user }}"
      group: "{{ user }}"
      timeout: "{{ item.1.timeout | default(3600) }}"
      force: false
    with_subelements:
      - "{{ llamacpp_models | dict2items }}"
      - value
    when: 
      - not (item.1.requires_auth | default(false))
      - item.1.priority <= 2
    tags: 
      - models
      - download
    retries: 3
    delay: 10

  - name: Create docker-compose.yml
    ansible.builtin.template:
      src: "{{ files }}/docker-compose.yml.j2"
      dest: "{{ home }}/docker-compose.yml"
      owner: "{{ user }}"
      group: "{{ user }}"
      mode: '0644'
    notify:
    - Reload systemd daemon
    - Restart llamacpp service

  - name: Create llamacpp environment file
    ansible.builtin.template:
      src: "{{ files }}/llamacpp.env.j2"
      dest: "{{ home }}/.env"
      owner: "{{ user }}"
      group: "{{ user }}"
      mode: '0600'
    notify:
    - Reload systemd daemon
    - Restart llamacpp service

  - name: Create llamacpp systemd service
    ansible.builtin.template:
      src: "{{ files }}/{{ item }}.j2"
      dest: "/etc/systemd/system/{{ item }}"
      mode: '0644'
    with_items:
    - llamacpp.service
    notify:
    - Reload systemd daemon
    - Restart llamacpp service

  - name: Enable llamacpp service
    ansible.builtin.systemd:
      name: llamacpp.service
      enabled: yes
      daemon_reload: yes

  handlers:
  - name: Reload systemd daemon
    ansible.builtin.systemd:
      daemon_reload: yes

  - name: Restart llamacpp service
    ansible.builtin.systemd:
      name: llamacpp.service
      state: restarted
