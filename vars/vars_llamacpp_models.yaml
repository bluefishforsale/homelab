# llamacpp Model Configuration for RTX 3090 (24GB VRAM)
# Optimized for single-user reasoning with max context in 16GB VRAM budget

llamacpp_models:
  reasoning:
    # Primary: Qwen3-14B - Optimal for 16GB VRAM with thinking/reasoning
    # Supports /think mode for step-by-step reasoning
    - name: "Qwen3-14B-Q4_K_M.gguf"
      url: "https://huggingface.co/unsloth/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q4_K_M.gguf"
      size: "8.5GB"
      parameters: "14B"
      vram_usage: "~9GB base + KV cache"
      capabilities: ["reasoning", "thinking", "step-by-step", "analysis", "math", "coding"]
      context_size: "40K"
      timeout: 3600
      priority: 1
      requires_auth: false
      
    # Fallback: Qwen3-8B for even larger context
    - name: "Qwen3-8B-Q4_K_M.gguf"
      url: "https://huggingface.co/unsloth/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf"
      size: "5.0GB"
      parameters: "8B"
      vram_usage: "~6GB base + KV cache"
      capabilities: ["reasoning", "thinking", "general", "analysis", "coding"]
      context_size: "64K"
      timeout: 2400
      priority: 2
      requires_auth: false
      
  coding:
    # Alternative: Code Llama for technical analysis
    - name: "CodeLlama-13B-Instruct-Q4_K_M.gguf"
      url: "https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.q4_k_m.gguf"
      size: "7.9GB"
      parameters: "13B"
      vram_usage: "~9GB"
      capabilities: ["coding", "technical_analysis", "debugging", "systems"]
      context_size: "16K"
      timeout: 3600
      priority: 2
      requires_auth: false

  large_reasoning:
    # For maximum reasoning capability (fits RTX 3090)
    - name: "Llama-2-70B-Chat-Q2_K.gguf"
      url: "https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q2_K.gguf"
      size: "26GB"
      parameters: "70B"
      vram_usage: "~23GB"
      capabilities: ["reasoning", "large_context", "general", "analysis"]
      context_size: "4K"
      timeout: 4800
      priority: 3
      requires_auth: false

  backup:
    # Current model as fallback (much smaller)
    - name: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
      url: "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
      size: "4GB"
      parameters: "7B"  
      vram_usage: "~5GB"
      capabilities: ["reasoning", "basic", "fast"]
      context_size: "16K"
      timeout: 1800
      priority: 4
      requires_auth: false

# RTX 3090 Performance Profiles (16GB VRAM budget for headroom)
rtx_3090_profiles:
  # Primary: Qwen3-14B with max context for single user
  single_user_reasoning:
    model: "Qwen3-14B-Q4_K_M.gguf"
    gpu_layers: -1
    context_size: 40960  # 40K context fits in 16GB with 14B model
    batch_size: 2048
    parallel_requests: 1  # Single user
    flash_attention: true
    thinking_mode: true
    
  # Alternative: Qwen3-8B with massive context
  max_context:
    model: "Qwen3-8B-Q4_K_M.gguf"  
    gpu_layers: -1
    context_size: 65536  # 64K context with 8B model
    batch_size: 2048
    parallel_requests: 1
    flash_attention: true
    thinking_mode: true
    
  # Fallback: smaller model for testing
  lightweight:
    model: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    gpu_layers: -1  
    context_size: 32768
    batch_size: 1024
    parallel_requests: 1
    flash_attention: true

# Model download priorities (1 = download first)
download_priority:
  1: "Qwen3-14B-Q4_K_M.gguf"                     # Primary reasoning model (fits 16GB)
  2: "Qwen3-8B-Q4_K_M.gguf"                      # Fallback with larger context
  3: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"   # Lightweight reasoning
